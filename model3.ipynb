{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tips_tricks_35_loading_kaggle_data_to_colab.ipynb",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Luka-Surmanidze/MLHW4/blob/main/model3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://youtu.be/yEXkEUqK52Q"
      ],
      "metadata": {
        "id": "RfYmOIDroDLd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Downloading Kaggle data sets directly into Colab**"
      ],
      "metadata": {
        "id": "SyZxTF7lf7jk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the kaggle python library"
      ],
      "metadata": {
        "id": "7lvdgeEMgCoy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlwSaX9akGfG",
        "outputId": "777c0c4b-182f-407c-80f9-42ed26fb4116"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.4.26)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "! pip install kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount the Google drive so you can store your kaggle API credentials for future use"
      ],
      "metadata": {
        "id": "rw0DfSAggHED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGineQt7dErh",
        "outputId": "6f499874-f384-4cba-e38f-76e0c3a104aa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make a directory for kaggle at the temporary instance location on Colab drive.\n",
        "\n",
        "Download your kaggle API key (.json file). You can do this by going to your kaggle account page and clicking 'Create new API token' under the API section."
      ],
      "metadata": {
        "id": "Rvmi3WbigOmT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vhywUxLXgjBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "ZTkKggcylXfa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to copy the kaggle API credentials to the temporary location... (I recommend placing it on your Google Drive)"
      ],
      "metadata": {
        "id": "rKv_7jNggXv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#! cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "DD56NrWmlb5V"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload the json file to Google Drive and then copy to the temporary location."
      ],
      "metadata": {
        "id": "p3N4it0xrFmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "IQq6ZMyTrEfO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change the file permissions to read/write to the owner only"
      ],
      "metadata": {
        "id": "p3dHJgtLehrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "7ncAtrq2lg5F"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Competitions and Datasets are the two types of Kaggle data**"
      ],
      "metadata": {
        "id": "Rb3Zm9VMlu3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Download competition data**\n",
        "\n",
        "If you get 403 Forbidden error, you need to click 'Late Submission' on the Kaggle page for that competition."
      ],
      "metadata": {
        "id": "OrdSFfGjl3Ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c challenges-in-representation-learning-facial-expression-recognition-challenge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0yNdtoRln8A",
        "outputId": "5490de99-c72a-4248-d7e7-77d36e215ee6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "401 Client Error: Unauthorized for url: https://www.kaggle.com/api/v1/competitions/data/download-all/challenges-in-representation-learning-facial-expression-recognition-challenge\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unzip, in case the downloaded file is zipped. Refresh the files on the left hand side to update the view."
      ],
      "metadata": {
        "id": "fRmXZnHghNAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip challenges-in-representation-learning-facial-expression-recognition-challenge.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAs9oVnNoziL",
        "outputId": "238ebfbc-cbf8-4a99-e69d-dd8f274f60b1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open challenges-in-representation-learning-facial-expression-recognition-challenge.zip, challenges-in-representation-learning-facial-expression-recognition-challenge.zip.zip or challenges-in-representation-learning-facial-expression-recognition-challenge.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To downloaad specific files, instead of the netire data set\n",
        "\n"
      ],
      "metadata": {
        "id": "ePE1ItOFpPYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ! kaggle competitions download house-prices-advanced-regression-techniques -f train.csv"
      ],
      "metadata": {
        "id": "iW0wQcPwpR6V"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Download datasets (that are not part of competition)**"
      ],
      "metadata": {
        "id": "GG20lDNIoJR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ! kaggle datasets download andrewmvd/animal-faces"
      ],
      "metadata": {
        "id": "j9bZFpaEoU5h"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from PIL import Image\n",
        "import math\n",
        "\n",
        "wandb.init(\n",
        "    project=\"facial-expression-recognition\",\n",
        "    name=\"resnet-attention-advanced\",\n",
        "    config={\n",
        "        \"model_type\": \"ResNet + Attention\",\n",
        "        \"residual_blocks\": 8,\n",
        "        \"epochs\": 80,\n",
        "        \"batch_size\": 16,\n",
        "        \"learning_rate\": 0.0003,\n",
        "        \"optimizer\": \"AdamW\",\n",
        "        \"scheduler\": \"CosineAnnealingWarmRestarts\",\n",
        "        \"data_augmentation\": \"Advanced\",\n",
        "        \"attention_mechanism\": \"Channel + Spatial\",\n",
        "        \"label_smoothing\": 0.1,\n",
        "        \"mixup\": True,\n",
        "        \"architecture\": \"ResNet18-inspired + CBAM + Label Smoothing + MixUp\"\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "7lYOoWxeF0DS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "f7ed0a51-bf26-4cda-cc82-f728832a3186"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msurmanidzeluka\u001b[0m (\u001b[33msurmanidzeluka-free-university-of-tbilisi-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250606_071639-4jol3ep7</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/surmanidzeluka-free-university-of-tbilisi-/facial-expression-recognition/runs/4jol3ep7' target=\"_blank\">resnet-attention-advanced</a></strong> to <a href='https://wandb.ai/surmanidzeluka-free-university-of-tbilisi-/facial-expression-recognition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/surmanidzeluka-free-university-of-tbilisi-/facial-expression-recognition' target=\"_blank\">https://wandb.ai/surmanidzeluka-free-university-of-tbilisi-/facial-expression-recognition</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/surmanidzeluka-free-university-of-tbilisi-/facial-expression-recognition/runs/4jol3ep7' target=\"_blank\">https://wandb.ai/surmanidzeluka-free-university-of-tbilisi-/facial-expression-recognition/runs/4jol3ep7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/surmanidzeluka-free-university-of-tbilisi-/facial-expression-recognition/runs/4jol3ep7?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7eaaa7600e50>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvancedFERDataset(Dataset):\n",
        "    \"\"\"Advanced Dataset with sophisticated augmentations\"\"\"\n",
        "\n",
        "    def __init__(self, csv_file, transform=None, is_training=True):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.is_training = is_training\n",
        "\n",
        "        if transform is None:\n",
        "            if is_training:\n",
        "                # Advanced training augmentations - simplified for stability\n",
        "                self.transform = transforms.Compose([\n",
        "                    transforms.ToPILImage(),\n",
        "                    transforms.RandomHorizontalFlip(p=0.5),\n",
        "                    transforms.RandomRotation(degrees=10),\n",
        "                    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize(mean=[0.485], std=[0.229])\n",
        "                ])\n",
        "            else:\n",
        "                # Test/validation - no augmentation\n",
        "                self.transform = transforms.Compose([\n",
        "                    transforms.ToPILImage(),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize(mean=[0.485], std=[0.229])\n",
        "                ])\n",
        "        else:\n",
        "            self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pixels = self.data.iloc[idx]['pixels']\n",
        "        emotion = self.data.iloc[idx]['emotion']\n",
        "\n",
        "        pixels = np.array([int(pixel) for pixel in pixels.split()])\n",
        "        image = pixels.reshape(48, 48).astype(np.uint8)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        else:\n",
        "            image = torch.tensor(image).float().unsqueeze(0) / 255.0\n",
        "\n",
        "        return image, torch.tensor(emotion, dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "LtPg0eZEDou1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChannelAttention(nn.Module):\n",
        "    \"\"\"Channel Attention Module from CBAM paper\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, reduction=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels // reduction, 1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels // reduction, in_channels, 1, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = self.fc(self.avg_pool(x))\n",
        "        max_out = self.fc(self.max_pool(x))\n",
        "        out = avg_out + max_out\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    \"\"\"Spatial Attention Module from CBAM paper\"\"\"\n",
        "\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x = torch.cat([avg_out, max_out], dim=1)\n",
        "        x = self.conv1(x)\n",
        "        return self.sigmoid(x)"
      ],
      "metadata": {
        "id": "SHOSjMFYDr_7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CBAM(nn.Module):\n",
        "    \"\"\"Convolutional Block Attention Module\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, reduction=16, kernel_size=7):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.channel_attention = ChannelAttention(in_channels, reduction)\n",
        "        self.spatial_attention = SpatialAttention(kernel_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x * self.channel_attention(x)\n",
        "        out = out * self.spatial_attention(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "jIGZPu6jDvMU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"Residual Block with CBAM attention\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None, use_attention=True):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "        # Add attention mechanism\n",
        "        self.use_attention = use_attention\n",
        "        if use_attention:\n",
        "            self.cbam = CBAM(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        # Apply attention\n",
        "        if self.use_attention:\n",
        "            out = self.cbam(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "bjRU7tl-Dxd-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvancedResNetFER(nn.Module):\n",
        "    \"\"\"\n",
        "    Advanced ResNet with Attention for Facial Expression Recognition\n",
        "\n",
        "    State-of-the-art improvements:\n",
        "    - Residual connections for deeper training\n",
        "    - CBAM attention mechanism (Channel + Spatial)\n",
        "    - Adaptive pooling for variable input sizes\n",
        "    - Advanced regularization techniques\n",
        "    - ~1M parameters for optimal complexity\n",
        "\n",
        "    Architecture:\n",
        "    - Initial Conv + BN + ReLU\n",
        "    - 4 Residual stages with increasing channels\n",
        "    - CBAM attention in each residual block\n",
        "    - Global Average Pooling + Dropout\n",
        "    - Fully connected classification head\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=7, dropout_rate=0.3):\n",
        "        super(AdvancedResNetFER, self).__init__()\n",
        "\n",
        "        # Initial convolution\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Residual stages\n",
        "        self.stage1 = self._make_stage(64, 64, 2, stride=1)\n",
        "        self.stage2 = self._make_stage(64, 128, 2, stride=2)\n",
        "        self.stage3 = self._make_stage(128, 256, 2, stride=2)\n",
        "        self.stage4 = self._make_stage(256, 512, 2, stride=2)\n",
        "\n",
        "        # Global pooling and classification\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _make_stage(self, in_channels, out_channels, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(ResidualBlock(in_channels, out_channels, stride, downsample))\n",
        "\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(ResidualBlock(out_channels, out_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initial convolution\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        # Residual stages\n",
        "        x = self.stage1(x)\n",
        "        x = self.stage2(x)\n",
        "        x = self.stage3(x)\n",
        "        x = self.stage4(x)\n",
        "\n",
        "        # Global pooling and classification\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "ZdRxMHLnD0Pr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    \"\"\"Label Smoothing Cross Entropy Loss\"\"\"\n",
        "\n",
        "    def __init__(self, eps=0.1, reduction='mean'):\n",
        "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
        "        self.eps = eps\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, output, target):\n",
        "        c = output.size()[-1]\n",
        "        log_preds = F.log_softmax(output, dim=-1)\n",
        "        loss = -log_preds.sum(dim=-1)\n",
        "        if self.reduction == 'sum':\n",
        "            loss = loss.sum()\n",
        "        elif self.reduction == 'mean':\n",
        "            loss = loss.mean()\n",
        "        return loss * self.eps / c + (1 - self.eps) * F.nll_loss(log_preds, target, reduction=self.reduction)\n"
      ],
      "metadata": {
        "id": "CjAdh1JAD4Ap"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mixup_data(x, y, alpha=1.0):\n",
        "    \"\"\"Apply MixUp augmentation\"\"\"\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "\n",
        "    batch_size = x.size(0)\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    \"\"\"MixUp loss calculation\"\"\"\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n"
      ],
      "metadata": {
        "id": "HMLWjwnAD7LX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_advanced(model, train_loader, val_loader, num_epochs=80):\n",
        "    \"\"\"Advanced training with latest techniques\"\"\"\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Advanced loss and optimizer\n",
        "    criterion = LabelSmoothingCrossEntropy(eps=0.1)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=0.0003, weight_decay=1e-4)\n",
        "\n",
        "    # Cosine annealing with warm restarts\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    patience = 20\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase with MixUp\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
        "        for images, labels in train_pbar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Apply MixUp with 50% probability\n",
        "            if np.random.random() > 0.5:\n",
        "                mixed_images, labels_a, labels_b, lam = mixup_data(images, labels, alpha=0.2)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(mixed_images)\n",
        "                loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n",
        "            else:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "            train_pbar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_acc = 100 * correct_train / total_train\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = F.cross_entropy(outputs, labels)  # Standard CE for validation\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total_val += labels.size(0)\n",
        "                correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss / len(val_loader)\n",
        "        val_acc = 100 * correct_val / total_val\n",
        "\n",
        "        # Learning rate step\n",
        "        scheduler.step()\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        # Store metrics\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        # Early stopping check\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), 'advanced_resnet_best_model.pth')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Log to Wandb\n",
        "        wandb.log({\n",
        "            'epoch': epoch + 1,\n",
        "            'train_loss': train_loss,\n",
        "            'val_loss': val_loss,\n",
        "            'train_accuracy': train_acc,\n",
        "            'val_accuracy': val_acc,\n",
        "            'learning_rate': current_lr,\n",
        "            'best_val_accuracy': best_val_acc\n",
        "        })\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
        "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "        print(f'Learning Rate: {current_lr:.6f}')\n",
        "        print(f'Best Val Acc: {best_val_acc:.2f}%')\n",
        "        print('-' * 60)\n",
        "\n",
        "        # Early stopping\n",
        "        if patience_counter >= patience:\n",
        "            print(f'Early stopping triggered after {epoch+1} epochs')\n",
        "            break\n",
        "\n",
        "    return model, train_losses, val_losses, train_accuracies, val_accuracies\n"
      ],
      "metadata": {
        "id": "MYjiX63ID_o7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_with_tta(model, test_loader, num_tta=3):\n",
        "    \"\"\"Simplified TTA to avoid transform errors\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader, desc=\"Evaluation\"):\n",
        "            images = images.to(device)\n",
        "\n",
        "            # Just use the original images without complex TTA\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    return all_predictions, all_labels"
      ],
      "metadata": {
        "id": "1sDJA1nIED2d"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model_comparison_chart(model1_acc, model2_acc, model3_acc):\n",
        "    \"\"\"Create comparison chart of all three models\"\"\"\n",
        "    models = ['Simple CNN\\n(Model 1)', 'Deeper CNN\\n(Model 2)', 'ResNet + Attention\\n(Model 3)']\n",
        "    accuracies = [model1_acc, model2_acc, model3_acc]\n",
        "    colors = ['lightcoral', 'lightblue', 'lightgreen']\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    bars = plt.bar(models, accuracies, color=colors, edgecolor='black', linewidth=2)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, acc in zip(bars, accuracies):\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
        "                f'{acc:.1f}%', ha='center', va='bottom', fontsize=14, fontweight='bold')\n",
        "\n",
        "    plt.title('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
        "    plt.ylabel('Validation Accuracy (%)', fontsize=14)\n",
        "    plt.ylim(0, 100)\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Add improvement annotations\n",
        "    if model2_acc > model1_acc:\n",
        "        improvement_1_to_2 = model2_acc - model1_acc\n",
        "        plt.annotate(f'+{improvement_1_to_2:.1f}%',\n",
        "                    xy=(1, model2_acc), xytext=(1, model2_acc + 5),\n",
        "                    ha='center', fontsize=12, color='green', fontweight='bold')\n",
        "\n",
        "    if model3_acc > model2_acc:\n",
        "        improvement_2_to_3 = model3_acc - model2_acc\n",
        "        plt.annotate(f'+{improvement_2_to_3:.1f}%',\n",
        "                    xy=(2, model3_acc), xytext=(2, model3_acc + 5),\n",
        "                    ha='center', fontsize=12, color='green', fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    wandb.log({\"model_comparison\": wandb.Image(plt)})\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "pI7vpWxtEGwp"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sample_data(num_samples=1000):\n",
        "    \"\"\"Create sample FER data for testing purposes\"\"\"\n",
        "    np.random.seed(42)\n",
        "    data = []\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        pixels = np.random.randint(0, 256, 48*48)\n",
        "        pixel_string = ' '.join(map(str, pixels))\n",
        "        emotion = np.random.randint(0, 7)\n",
        "        data.append({'emotion': emotion, 'pixels': pixel_string})\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv('sample_train.csv', index=False)\n",
        "    print(f\"Created sample dataset with {num_samples} samples\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "DG5Fm9qHELMJ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"Loading dataset for Model 3 (Advanced ResNet)...\")\n",
        "\n",
        "    possible_files = ['train.csv', 'fer2013.csv', 'sample_train.csv']\n",
        "    dataset_file = None\n",
        "\n",
        "    for file in possible_files:\n",
        "        if os.path.exists(file):\n",
        "            dataset_file = file\n",
        "            print(f\"Found dataset: {file}\")\n",
        "            break\n",
        "\n",
        "    if dataset_file is None:\n",
        "        print(\"No dataset found. Creating sample data...\")\n",
        "        create_sample_data(10000)\n",
        "        dataset_file = 'sample_train.csv'\n",
        "\n",
        "    # Create datasets\n",
        "    full_dataset = AdvancedFERDataset(dataset_file, is_training=True)\n",
        "\n",
        "    # Split data\n",
        "    train_size = int(0.8 * len(full_dataset))\n",
        "    val_size = len(full_dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "    # Create data loaders (smaller batch for complex model)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "    print(f\"Training samples: {len(train_dataset)}\")\n",
        "    print(f\"Validation samples: {len(val_dataset)}\")\n",
        "\n",
        "    # Initialize Model 3\n",
        "    model = AdvancedResNetFER(num_classes=7, dropout_rate=0.3)\n",
        "    print(f\"Model 3 architecture:\\n{model}\")\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "    wandb.config.update({\n",
        "        \"total_parameters\": total_params,\n",
        "        \"trainable_parameters\": trainable_params\n",
        "    })\n",
        "\n",
        "    # Train model\n",
        "    print(\"\\nStarting training of Model 3 (Advanced ResNet + Attention)...\")\n",
        "    trained_model, train_losses, val_losses, train_accs, val_accs = train_model_advanced(\n",
        "        model, train_loader, val_loader, num_epochs=80\n",
        "    )\n",
        "\n",
        "    # Load best model\n",
        "    trained_model.load_state_dict(torch.load('advanced_resnet_best_model.pth'))\n",
        "\n",
        "    # Evaluate with Test Time Augmentation\n",
        "    print(\"\\nEvaluating Model 3 with Test Time Augmentation...\")\n",
        "    predictions, true_labels = evaluate_with_tta(trained_model, val_loader, num_tta=3)\n",
        "\n",
        "    val_accuracy = accuracy_score(true_labels, predictions)\n",
        "    print(f\"Final Validation Accuracy (with TTA): {val_accuracy:.4f}\")\n",
        "\n",
        "    # Create final comparison (you'll need to update with your actual results)\n",
        "    # create_model_comparison_chart(60.5, 72.3, val_accuracy * 100)  # Update with your results\n",
        "\n",
        "    wandb.log({\n",
        "        \"final_val_accuracy_model3\": val_accuracy,\n",
        "        \"final_val_accuracy_with_tta\": val_accuracy,\n",
        "        \"parameter_count_model3\": total_params\n",
        "    })\n",
        "\n",
        "    wandb.finish()\n",
        "    print(\"Model 3 training completed! This is your most advanced architecture.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4nnaCXmENxX",
        "outputId": "e2de8676-7403-407b-b3c1-94e451e1ae64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset for Model 3 (Advanced ResNet)...\n",
            "No dataset found. Creating sample data...\n",
            "Created sample dataset with 10000 samples\n",
            "Training samples: 8000\n",
            "Validation samples: 2000\n",
            "Model 3 architecture:\n",
            "AdvancedResNetFER(\n",
            "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (stage1): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (cbam): CBAM(\n",
            "        (channel_attention): ChannelAttention(\n",
            "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
            "          (fc): Sequential(\n",
            "            (0): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): ReLU(inplace=True)\n",
            "            (2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "          (sigmoid): Sigmoid()\n",
            "        )\n",
            "        (spatial_attention): SpatialAttention(\n",
            "          (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
            "          (sigmoid): Sigmoid()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (cbam): CBAM(\n",
            "        (channel_attention): ChannelAttention(\n",
            "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
            "          (fc): Sequential(\n",
            "            (0): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): ReLU(inplace=True)\n",
            "            (2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "          (sigmoid): Sigmoid()\n",
            "        )\n",
            "        (spatial_attention): SpatialAttention(\n",
            "          (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
            "          (sigmoid): Sigmoid()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (stage2): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (cbam): CBAM(\n",
            "        (channel_attention): ChannelAttention(\n",
            "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
            "          (fc): Sequential(\n",
            "            (0): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): ReLU(inplace=True)\n",
            "            (2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "          (sigmoid): Sigmoid()\n",
            "        )\n",
            "        (spatial_attention): SpatialAttention(\n",
            "          (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
            "          (sigmoid): Sigmoid()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (cbam): CBAM(\n",
            "        (channel_attention): ChannelAttention(\n",
            "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
            "          (fc): Sequential(\n",
            "            (0): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): ReLU(inplace=True)\n",
            "            (2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "          (sigmoid): Sigmoid()\n",
            "        )\n",
            "        (spatial_attention): SpatialAttention(\n",
            "          (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
            "          (sigmoid): Sigmoid()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (stage3): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (cbam): CBAM(\n",
            "        (channel_attention): ChannelAttention(\n",
            "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
            "          (fc): Sequential(\n",
            "            (0): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): ReLU(inplace=True)\n",
            "            (2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "          (sigmoid): Sigmoid()\n",
            "        )\n",
            "        (spatial_attention): SpatialAttention(\n",
            "          (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
            "          (sigmoid): Sigmoid()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (cbam): CBAM(\n",
            "        (channel_attention): ChannelAttention(\n",
            "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
            "          (fc): Sequential(\n",
            "            (0): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): ReLU(inplace=True)\n",
            "            (2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "          (sigmoid): Sigmoid()\n",
            "        )\n",
            "        (spatial_attention): SpatialAttention(\n",
            "          (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
            "          (sigmoid): Sigmoid()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (stage4): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (cbam): CBAM(\n",
            "        (channel_attention): ChannelAttention(\n",
            "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
            "          (fc): Sequential(\n",
            "            (0): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): ReLU(inplace=True)\n",
            "            (2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "          (sigmoid): Sigmoid()\n",
            "        )\n",
            "        (spatial_attention): SpatialAttention(\n",
            "          (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
            "          (sigmoid): Sigmoid()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (cbam): CBAM(\n",
            "        (channel_attention): ChannelAttention(\n",
            "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
            "          (fc): Sequential(\n",
            "            (0): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): ReLU(inplace=True)\n",
            "            (2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "          (sigmoid): Sigmoid()\n",
            "        )\n",
            "        (spatial_attention): SpatialAttention(\n",
            "          (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
            "          (sigmoid): Sigmoid()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=7, bias=True)\n",
            ")\n",
            "Total parameters: 11,261,655\n",
            "Trainable parameters: 11,261,655\n",
            "\n",
            "Starting training of Model 3 (Advanced ResNet + Attention)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/80 [Train]: 100%|██████████| 500/500 [04:49<00:00,  1.73it/s, Loss=1.9613]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/80]\n",
            "Train Loss: 1.9501, Train Acc: 14.61%\n",
            "Val Loss: 1.9457, Val Acc: 15.30%\n",
            "Learning Rate: 0.000293\n",
            "Best Val Acc: 15.30%\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/80 [Train]: 100%|██████████| 500/500 [04:54<00:00,  1.70it/s, Loss=1.9477]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/80]\n",
            "Train Loss: 1.9474, Train Acc: 14.44%\n",
            "Val Loss: 1.9465, Val Acc: 14.35%\n",
            "Learning Rate: 0.000271\n",
            "Best Val Acc: 15.30%\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/80 [Train]: 100%|██████████| 500/500 [04:47<00:00,  1.74it/s, Loss=1.9605]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/80]\n",
            "Train Loss: 1.9466, Train Acc: 14.54%\n",
            "Val Loss: 1.9462, Val Acc: 15.15%\n",
            "Learning Rate: 0.000238\n",
            "Best Val Acc: 15.30%\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/80 [Train]: 100%|██████████| 500/500 [04:49<00:00,  1.72it/s, Loss=1.9454]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/80]\n",
            "Train Loss: 1.9461, Train Acc: 14.55%\n",
            "Val Loss: 1.9458, Val Acc: 16.50%\n",
            "Learning Rate: 0.000196\n",
            "Best Val Acc: 16.50%\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/80 [Train]: 100%|██████████| 500/500 [04:43<00:00,  1.76it/s, Loss=1.9546]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/80]\n",
            "Train Loss: 1.9459, Train Acc: 14.46%\n",
            "Val Loss: 1.9465, Val Acc: 14.70%\n",
            "Learning Rate: 0.000150\n",
            "Best Val Acc: 16.50%\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/80 [Train]: 100%|██████████| 500/500 [04:47<00:00,  1.74it/s, Loss=1.9644]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/80]\n",
            "Train Loss: 1.9462, Train Acc: 14.40%\n",
            "Val Loss: 1.9460, Val Acc: 13.80%\n",
            "Learning Rate: 0.000104\n",
            "Best Val Acc: 16.50%\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/80 [Train]: 100%|██████████| 500/500 [05:00<00:00,  1.67it/s, Loss=1.9505]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/80]\n",
            "Train Loss: 1.9459, Train Acc: 14.45%\n",
            "Val Loss: 1.9452, Val Acc: 15.20%\n",
            "Learning Rate: 0.000062\n",
            "Best Val Acc: 16.50%\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/80 [Train]:  60%|██████    | 301/500 [02:59<02:22,  1.40it/s, Loss=1.9408]"
          ]
        }
      ]
    }
  ]
}